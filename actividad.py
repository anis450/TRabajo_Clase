# -*- coding: utf-8 -*-
"""Actividad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eiReUuI-F_fJiVvEIPVLpCbPFL8spAg8
"""

#Cargar datos y exploración


# Instalación de librerías si no están (solo primera vez en Colab)
!pip install seaborn scikit-learn

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Cargar dataset desde seaborn (evita problemas con CSV)
data = sns.load_dataset("diamonds")
print(data.head())
print(data.describe())

# Exploración simple
plt.figure(figsize=(6,4))
plt.scatter(data["carat"], data["price"], alpha=0.3, s=5)
plt.xlabel("Carat")
plt.ylabel("Price")
plt.title("Diamonds: Carat vs Price")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Usamos 'carat' como predictor
X = data[["carat"]]
y = data["price"]

# Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Predicciones
y_pred = model.predict(X_test)

# Evaluación
print(" Resultados con Scikit-learn")
print("Coeficiente (pendiente):", model.coef_[0])
print("Intercepto:", model.intercept_)
print("MSE:", mean_squared_error(y_test, y_pred))
print("R²:", r2_score(y_test, y_pred))

import numpy as np

class LinearRegressionGD:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.n_iterations):
            y_pred = np.dot(X, self.weights) + self.bias
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)

            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias


# Normalizar X para mejor convergencia
X_norm = (X - X.mean()) / X.std()

gd_model = LinearRegressionGD(learning_rate=0.01, n_iterations=1000)
gd_model.fit(X_norm, y)

y_pred_gd = gd_model.predict(X_norm)

print("\n Resultados con implementación manual")
print("Peso (coef):", gd_model.weights)
print("Bias:", gd_model.bias)

## Discusión:
- El modelo de Scikit-learn es mucho más rápido y directo, solo se entrena con `.fit()` y ya ofrece métricas de evaluación.
- El modelo manual con gradiente descendente muestra cómo funciona internamente el algoritmo: va ajustando pesos y bias paso a paso.
- Sin normalización, el gradiente descendente puede no converger.
- Con pocas features (ejemplo: solo `carat`), ambos modelos se comportan parecido.
- Para datasets grandes, Scikit-learn es más eficiente y robusto, mientras que el modelo manual es más educativo.

import numpy as np

class LinearRegressionPOO:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.n_iterations):
            y_pred = np.dot(X, self.weights) + self.bias
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)

            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

    def score(self, X, y):
        y_pred = self.predict(X)
        ss_total = np.sum((y - np.mean(y))**2)
        ss_res = np.sum((y - y_pred)**2)
        return 1 - (ss_res / ss_total)


# Uso del modelo POO
X = data[["carat"]].values
y = data["price"].values

model_poo = LinearRegressionPOO(learning_rate=0.01, n_iterations=1000)
model_poo.fit(X, y)

print(" Resultados con el modelo POO")
print("Peso:", model_poo.weights)
print("Bias:", model_poo.bias)
print("R²:", model_poo.score(X, y))